{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "un_sup.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgb-NDlN836H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "008fe55d-b01c-45ac-d922-d5c74cae4e54"
      },
      "source": [
        "from keras.datasets import fashion_mnist"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLTGiSKREABC",
        "colab_type": "text"
      },
      "source": [
        "**Unsupervised ML to learn about data!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3WKhNVk9D3D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "925b3455-92c5-4956-e2d6-f65dfdce71a9"
      },
      "source": [
        "%pylab inline\n",
        "import os\n",
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras.backend as K\n",
        "\n",
        "from time import time\n",
        "from sklearn.cluster import KMeans\n",
        "from keras import callbacks\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import Dense, Input\n",
        "from keras.initializers import VarianceScaling\n",
        "from keras.engine.topology import Layer, InputSpec\n",
        "\n",
        "from scipy.misc import imread\n",
        "from sklearn.metrics import accuracy_score, normalized_mutual_info_score"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkFNwXpq9LSH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "5630e7a5-5f60-416f-c94e-7606d0395099"
      },
      "source": [
        "(train_x, train_y), (val_x, val_y) = fashion_mnist.load_data()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 3us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 2s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmomSM979OcV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x = train_x/255.\n",
        "val_x = val_x/255.\n",
        "\n",
        "train_x = train_x.reshape(-1, 784)\n",
        "val_x = val_x.reshape(-1, 784)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "398g58g-9Tac",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "c05dfa58-819b-48af-f32b-0f6c4b4e5377"
      },
      "source": [
        "# this is our input placeholder\n",
        "input_img = Input(shape=(784,))\n",
        "\n",
        "# \"encoded\" is the encoded representation of the input\n",
        "encoded = Dense(2000, activation='relu')(input_img)\n",
        "encoded = Dense(500, activation='relu')(encoded)\n",
        "encoded = Dense(500, activation='relu')(encoded)\n",
        "encoded = Dense(10, activation='sigmoid')(encoded)\n",
        "\n",
        "# \"decoded\" is the lossy reconstruction of the input\n",
        "decoded = Dense(500, activation='relu')(encoded)\n",
        "decoded = Dense(500, activation='relu')(decoded)\n",
        "decoded = Dense(2000, activation='relu')(decoded)\n",
        "decoded = Dense(784)(decoded)\n",
        "\n",
        "# this model maps an input to its reconstruction\n",
        "autoencoder = Model(input_img, decoded)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAd5m3s89dWy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "733f0487-94ca-48c4-e9be-c7cd1fd8bd4b"
      },
      "source": [
        "autoencoder.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2000)              1570000   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 500)               1000500   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                5010      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 500)               5500      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 2000)              1002000   \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 784)               1568784   \n",
            "=================================================================\n",
            "Total params: 5,652,794\n",
            "Trainable params: 5,652,794\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfE0LBYv9gj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  this model maps an input to its encoded representation\n",
        "encoder = Model(input_img, encoded)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfX32F8S9kiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "autoencoder.compile(optimizer='adam', loss='mse')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQeOSfkZ9nTV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9LOx7639qY4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2069
        },
        "outputId": "b48e1df9-f0ba-4548-d812-1d0752b784c0"
      },
      "source": [
        "train_history = autoencoder.fit(train_x, train_x, epochs=500, batch_size=2048, validation_data=(val_x, val_x), callbacks=[estop])\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/500\n",
            "60000/60000 [==============================] - 5s 80us/step - loss: 0.0916 - val_loss: 0.0688\n",
            "Epoch 2/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0585 - val_loss: 0.0463\n",
            "Epoch 3/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0419 - val_loss: 0.0386\n",
            "Epoch 4/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0369 - val_loss: 0.0348\n",
            "Epoch 5/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0331 - val_loss: 0.0306\n",
            "Epoch 6/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0295 - val_loss: 0.0289\n",
            "Epoch 7/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0265 - val_loss: 0.0251\n",
            "Epoch 8/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0244 - val_loss: 0.0238\n",
            "Epoch 9/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0232 - val_loss: 0.0228\n",
            "Epoch 10/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0224 - val_loss: 0.0219\n",
            "Epoch 11/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0215 - val_loss: 0.0213\n",
            "Epoch 12/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0206 - val_loss: 0.0203\n",
            "Epoch 13/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0202 - val_loss: 0.0197\n",
            "Epoch 14/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0195 - val_loss: 0.0194\n",
            "Epoch 15/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0192 - val_loss: 0.0190\n",
            "Epoch 16/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0187 - val_loss: 0.0194\n",
            "Epoch 17/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0183 - val_loss: 0.0182\n",
            "Epoch 18/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0179 - val_loss: 0.0178\n",
            "Epoch 19/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0175 - val_loss: 0.0185\n",
            "Epoch 20/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0173 - val_loss: 0.0174\n",
            "Epoch 21/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0170 - val_loss: 0.0167\n",
            "Epoch 22/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0167 - val_loss: 0.0166\n",
            "Epoch 23/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0162 - val_loss: 0.0161\n",
            "Epoch 24/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0159 - val_loss: 0.0159\n",
            "Epoch 25/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0161 - val_loss: 0.0157\n",
            "Epoch 26/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0153 - val_loss: 0.0154\n",
            "Epoch 27/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0153 - val_loss: 0.0152\n",
            "Epoch 28/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0150 - val_loss: 0.0153\n",
            "Epoch 29/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0149 - val_loss: 0.0154\n",
            "Epoch 30/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0148 - val_loss: 0.0155\n",
            "Epoch 31/500\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0146 - val_loss: 0.0147\n",
            "Epoch 32/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0146 - val_loss: 0.0147\n",
            "Epoch 33/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0143 - val_loss: 0.0145\n",
            "Epoch 34/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0143 - val_loss: 0.0143\n",
            "Epoch 35/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0143 - val_loss: 0.0150\n",
            "Epoch 36/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0140 - val_loss: 0.0142\n",
            "Epoch 37/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0138 - val_loss: 0.0143\n",
            "Epoch 38/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0140 - val_loss: 0.0139\n",
            "Epoch 39/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0138 - val_loss: 0.0140\n",
            "Epoch 40/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0136 - val_loss: 0.0139\n",
            "Epoch 41/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0136 - val_loss: 0.0137\n",
            "Epoch 42/500\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0137 - val_loss: 0.0138\n",
            "Epoch 43/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0133 - val_loss: 0.0141\n",
            "Epoch 44/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0133 - val_loss: 0.0135\n",
            "Epoch 45/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0133 - val_loss: 0.0134\n",
            "Epoch 46/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0133 - val_loss: 0.0136\n",
            "Epoch 47/500\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0130 - val_loss: 0.0133\n",
            "Epoch 48/500\n",
            "47104/60000 [======================>.......] - ETA: 0s - loss: 0.0130"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-2674d1d6f014>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnsBG0zx9sfE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = autoencoder.predict(val_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3MB7Uwq9-9r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "b3359744-b1c1-4f0f-8392-8e9b1b22edb7"
      },
      "source": [
        "plt.imshow(pred[0].reshape(28, 28), cmap='gray')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fb1c64f7ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzGhUkn--BO7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "7eca3766-d4a2-45e4-8f6b-a033d798b99b"
      },
      "source": [
        "plt.imshow(val_x[0].reshape(28, 28), cmap='gray')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fb1c640bb38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWmqmZk3-GTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import fashion_mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLZcanV0-XxQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "52f84c35-4b71-42f2-ab0d-ab2c5ad9ae29"
      },
      "source": [
        "%pylab inline\n",
        "import os\n",
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras.backend as K\n",
        "from keras import regularizers\n",
        "\n",
        "\n",
        "from time import time\n",
        "from sklearn.cluster import KMeans\n",
        "from keras import callbacks\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import Dense, Input\n",
        "from keras.initializers import VarianceScaling\n",
        "from keras.engine.topology import Layer, InputSpec\n",
        "\n",
        "from scipy.misc import imread\n",
        "from sklearn.metrics import accuracy_score, normalized_mutual_info_score\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['imread', 'time']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhFhuarA-bxQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(train_x, train_y), (val_x, val_y) = fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLvsoIby-eul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x = train_x/255.\n",
        "val_x = val_x/255.\n",
        "\n",
        "train_x = train_x.reshape(-1, 784)\n",
        "val_x = val_x.reshape(-1, 784)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtPLtKre-grN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this is our input placeholder\n",
        "input_img = Input(shape=(784,))\n",
        "\n",
        "# \"encoded\" is the encoded representation of the input\n",
        "encoded = Dense(2000, activation='relu')(input_img)\n",
        "encoded = Dense(500, activation='relu',\n",
        "                activity_regularizer=regularizers.l1(10e-10))(encoded)\n",
        "encoded = Dense(500, activation='relu',\n",
        "                activity_regularizer=regularizers.l1(10e-10))(encoded)\n",
        "encoded = Dense(10, activation='sigmoid',\n",
        "                activity_regularizer=regularizers.l1(10e-10))(encoded)\n",
        "\n",
        "# \"decoded\" is the lossy reconstruction of the input\n",
        "decoded = Dense(500, activation='relu')(encoded)\n",
        "decoded = Dense(500, activation='relu')(decoded)\n",
        "decoded = Dense(2000, activation='relu')(decoded)\n",
        "decoded = Dense(784)(decoded)\n",
        "\n",
        "# this model maps an input to its reconstruction\n",
        "autoencoder = Model(input_img, decoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MBNHhz3-kmR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "d2b57bcf-2639-4d3b-e7d9-ca100936a058"
      },
      "source": [
        "autoencoder.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 2000)              1570000   \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 500)               1000500   \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 10)                5010      \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 500)               5500      \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 2000)              1002000   \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 784)               1568784   \n",
            "=================================================================\n",
            "Total params: 5,652,794\n",
            "Trainable params: 5,652,794\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kTl7K2C-mal",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  this model maps an input to its encoded representation\n",
        "encoder = Model(input_img, encoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkkCAA28-opk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "autoencoder.compile(optimizer='adam', loss='mse')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1KI5Aa7-qb1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnFNWBfd-s4N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6981
        },
        "outputId": "de767dbe-e061-4606-c8a1-beb7e64ab07d"
      },
      "source": [
        "train_history = autoencoder.fit(train_x, train_x, epochs=200, batch_size=2048, validation_data=(val_x, val_x), callbacks=[estop])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/200\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0913 - val_loss: 0.0678\n",
            "Epoch 2/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0610 - val_loss: 0.0519\n",
            "Epoch 3/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0471 - val_loss: 0.0407\n",
            "Epoch 4/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0385 - val_loss: 0.0366\n",
            "Epoch 5/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0358 - val_loss: 0.0344\n",
            "Epoch 6/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0326 - val_loss: 0.0307\n",
            "Epoch 7/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0295 - val_loss: 0.0282\n",
            "Epoch 8/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0265 - val_loss: 0.0255\n",
            "Epoch 9/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0251 - val_loss: 0.0245\n",
            "Epoch 10/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0239 - val_loss: 0.0238\n",
            "Epoch 11/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0234 - val_loss: 0.0229\n",
            "Epoch 12/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0221 - val_loss: 0.0218\n",
            "Epoch 13/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0216 - val_loss: 0.0212\n",
            "Epoch 14/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0207 - val_loss: 0.0206\n",
            "Epoch 15/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0203 - val_loss: 0.0203\n",
            "Epoch 16/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0202 - val_loss: 0.0197\n",
            "Epoch 17/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0195 - val_loss: 0.0193\n",
            "Epoch 18/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0195 - val_loss: 0.0191\n",
            "Epoch 19/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0188 - val_loss: 0.0196\n",
            "Epoch 20/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0186 - val_loss: 0.0184\n",
            "Epoch 21/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0183 - val_loss: 0.0183\n",
            "Epoch 22/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0180 - val_loss: 0.0178\n",
            "Epoch 23/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0175 - val_loss: 0.0175\n",
            "Epoch 24/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0173 - val_loss: 0.0173\n",
            "Epoch 25/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0168 - val_loss: 0.0170\n",
            "Epoch 26/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0166 - val_loss: 0.0167\n",
            "Epoch 27/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0165 - val_loss: 0.0164\n",
            "Epoch 28/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0160 - val_loss: 0.0160\n",
            "Epoch 29/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0160 - val_loss: 0.0159\n",
            "Epoch 30/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0156 - val_loss: 0.0157\n",
            "Epoch 31/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0156 - val_loss: 0.0156\n",
            "Epoch 32/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0153 - val_loss: 0.0154\n",
            "Epoch 33/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0154 - val_loss: 0.0154\n",
            "Epoch 34/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0151 - val_loss: 0.0155\n",
            "Epoch 35/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0149 - val_loss: 0.0150\n",
            "Epoch 36/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0149 - val_loss: 0.0155\n",
            "Epoch 37/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0147 - val_loss: 0.0148\n",
            "Epoch 38/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0146 - val_loss: 0.0147\n",
            "Epoch 39/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0145 - val_loss: 0.0147\n",
            "Epoch 40/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0143 - val_loss: 0.0145\n",
            "Epoch 41/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0143 - val_loss: 0.0144\n",
            "Epoch 42/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0142 - val_loss: 0.0147\n",
            "Epoch 43/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0140 - val_loss: 0.0142\n",
            "Epoch 44/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0139 - val_loss: 0.0141\n",
            "Epoch 45/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0138 - val_loss: 0.0146\n",
            "Epoch 46/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0139 - val_loss: 0.0139\n",
            "Epoch 47/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0135 - val_loss: 0.0139\n",
            "Epoch 48/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0136 - val_loss: 0.0138\n",
            "Epoch 49/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0135 - val_loss: 0.0144\n",
            "Epoch 50/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0135 - val_loss: 0.0139\n",
            "Epoch 51/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0133 - val_loss: 0.0136\n",
            "Epoch 52/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0132 - val_loss: 0.0136\n",
            "Epoch 53/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0133 - val_loss: 0.0136\n",
            "Epoch 54/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0131 - val_loss: 0.0137\n",
            "Epoch 55/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0131 - val_loss: 0.0134\n",
            "Epoch 56/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0130 - val_loss: 0.0140\n",
            "Epoch 57/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0131 - val_loss: 0.0132\n",
            "Epoch 58/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0129 - val_loss: 0.0132\n",
            "Epoch 59/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0129 - val_loss: 0.0132\n",
            "Epoch 60/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0128 - val_loss: 0.0132\n",
            "Epoch 61/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0129 - val_loss: 0.0131\n",
            "Epoch 62/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0126 - val_loss: 0.0130\n",
            "Epoch 63/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0125 - val_loss: 0.0129\n",
            "Epoch 64/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0128 - val_loss: 0.0130\n",
            "Epoch 65/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0125 - val_loss: 0.0129\n",
            "Epoch 66/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0124 - val_loss: 0.0130\n",
            "Epoch 67/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0126 - val_loss: 0.0129\n",
            "Epoch 68/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0123 - val_loss: 0.0128\n",
            "Epoch 69/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0127 - val_loss: 0.0128\n",
            "Epoch 70/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0123 - val_loss: 0.0127\n",
            "Epoch 71/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0122 - val_loss: 0.0127\n",
            "Epoch 72/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0123 - val_loss: 0.0128\n",
            "Epoch 73/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0122 - val_loss: 0.0127\n",
            "Epoch 74/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0122 - val_loss: 0.0127\n",
            "Epoch 75/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0122 - val_loss: 0.0127\n",
            "Epoch 76/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0121 - val_loss: 0.0126\n",
            "Epoch 77/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0120 - val_loss: 0.0127\n",
            "Epoch 78/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0121 - val_loss: 0.0125\n",
            "Epoch 79/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0120 - val_loss: 0.0125\n",
            "Epoch 80/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0120 - val_loss: 0.0125\n",
            "Epoch 81/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0120 - val_loss: 0.0125\n",
            "Epoch 82/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0119 - val_loss: 0.0124\n",
            "Epoch 83/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0120 - val_loss: 0.0124\n",
            "Epoch 84/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0118 - val_loss: 0.0124\n",
            "Epoch 85/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0118 - val_loss: 0.0124\n",
            "Epoch 86/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0119 - val_loss: 0.0127\n",
            "Epoch 87/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0118 - val_loss: 0.0125\n",
            "Epoch 88/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0118 - val_loss: 0.0124\n",
            "Epoch 89/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0117 - val_loss: 0.0123\n",
            "Epoch 90/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0116 - val_loss: 0.0123\n",
            "Epoch 91/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0118 - val_loss: 0.0123\n",
            "Epoch 92/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0117 - val_loss: 0.0124\n",
            "Epoch 93/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0116 - val_loss: 0.0122\n",
            "Epoch 94/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0115 - val_loss: 0.0123\n",
            "Epoch 95/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0117 - val_loss: 0.0123\n",
            "Epoch 96/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0115 - val_loss: 0.0122\n",
            "Epoch 97/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0118 - val_loss: 0.0123\n",
            "Epoch 98/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0114 - val_loss: 0.0122\n",
            "Epoch 99/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0114 - val_loss: 0.0122\n",
            "Epoch 100/200\n",
            "60000/60000 [==============================] - 1s 25us/step - loss: 0.0114 - val_loss: 0.0121\n",
            "Epoch 101/200\n",
            "60000/60000 [==============================] - 1s 25us/step - loss: 0.0117 - val_loss: 0.0130\n",
            "Epoch 102/200\n",
            "60000/60000 [==============================] - 1s 25us/step - loss: 0.0117 - val_loss: 0.0121\n",
            "Epoch 103/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0113 - val_loss: 0.0120\n",
            "Epoch 104/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0113 - val_loss: 0.0123\n",
            "Epoch 105/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0115 - val_loss: 0.0120\n",
            "Epoch 106/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0112 - val_loss: 0.0120\n",
            "Epoch 107/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0113 - val_loss: 0.0120\n",
            "Epoch 108/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0113 - val_loss: 0.0121\n",
            "Epoch 109/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0112 - val_loss: 0.0120\n",
            "Epoch 110/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0113 - val_loss: 0.0120\n",
            "Epoch 111/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0112 - val_loss: 0.0120\n",
            "Epoch 112/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0112 - val_loss: 0.0121\n",
            "Epoch 113/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0112 - val_loss: 0.0120\n",
            "Epoch 114/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0111 - val_loss: 0.0119\n",
            "Epoch 115/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0114 - val_loss: 0.0120\n",
            "Epoch 116/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0111 - val_loss: 0.0119\n",
            "Epoch 117/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0111 - val_loss: 0.0119\n",
            "Epoch 118/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0111 - val_loss: 0.0122\n",
            "Epoch 119/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0111 - val_loss: 0.0119\n",
            "Epoch 120/200\n",
            "60000/60000 [==============================] - 1s 25us/step - loss: 0.0110 - val_loss: 0.0121\n",
            "Epoch 121/200\n",
            "60000/60000 [==============================] - 1s 25us/step - loss: 0.0111 - val_loss: 0.0122\n",
            "Epoch 122/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0111 - val_loss: 0.0119\n",
            "Epoch 123/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0111 - val_loss: 0.0118\n",
            "Epoch 124/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0109 - val_loss: 0.0119\n",
            "Epoch 125/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0110 - val_loss: 0.0122\n",
            "Epoch 126/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0110 - val_loss: 0.0118\n",
            "Epoch 127/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0110 - val_loss: 0.0118\n",
            "Epoch 128/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0109 - val_loss: 0.0119\n",
            "Epoch 129/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0109 - val_loss: 0.0119\n",
            "Epoch 130/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0110 - val_loss: 0.0118\n",
            "Epoch 131/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0109 - val_loss: 0.0117\n",
            "Epoch 132/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0108 - val_loss: 0.0118\n",
            "Epoch 133/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0109 - val_loss: 0.0121\n",
            "Epoch 134/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0108 - val_loss: 0.0117\n",
            "Epoch 135/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0109 - val_loss: 0.0118\n",
            "Epoch 136/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0108 - val_loss: 0.0118\n",
            "Epoch 137/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0108 - val_loss: 0.0117\n",
            "Epoch 138/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0107 - val_loss: 0.0122\n",
            "Epoch 139/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0110 - val_loss: 0.0117\n",
            "Epoch 140/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0107 - val_loss: 0.0117\n",
            "Epoch 141/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0107 - val_loss: 0.0118\n",
            "Epoch 142/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0110 - val_loss: 0.0118\n",
            "Epoch 143/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0107 - val_loss: 0.0117\n",
            "Epoch 144/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0106 - val_loss: 0.0119\n",
            "Epoch 145/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0107 - val_loss: 0.0119\n",
            "Epoch 146/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0107 - val_loss: 0.0118\n",
            "Epoch 147/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0107 - val_loss: 0.0117\n",
            "Epoch 148/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0107 - val_loss: 0.0117\n",
            "Epoch 149/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0106 - val_loss: 0.0116\n",
            "Epoch 150/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0107 - val_loss: 0.0117\n",
            "Epoch 151/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0106 - val_loss: 0.0116\n",
            "Epoch 152/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0106 - val_loss: 0.0116\n",
            "Epoch 153/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0106 - val_loss: 0.0116\n",
            "Epoch 154/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0107 - val_loss: 0.0116\n",
            "Epoch 155/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0106 - val_loss: 0.0118\n",
            "Epoch 156/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0106 - val_loss: 0.0116\n",
            "Epoch 157/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0106 - val_loss: 0.0119\n",
            "Epoch 158/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0106 - val_loss: 0.0116\n",
            "Epoch 159/200\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0104 - val_loss: 0.0116\n",
            "Epoch 160/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0106 - val_loss: 0.0116\n",
            "Epoch 161/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0104 - val_loss: 0.0116\n",
            "Epoch 162/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0106 - val_loss: 0.0117\n",
            "Epoch 163/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0105 - val_loss: 0.0116\n",
            "Epoch 164/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0104 - val_loss: 0.0117\n",
            "Epoch 165/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0105 - val_loss: 0.0116\n",
            "Epoch 166/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0104 - val_loss: 0.0116\n",
            "Epoch 167/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0107 - val_loss: 0.0117\n",
            "Epoch 168/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0104 - val_loss: 0.0115\n",
            "Epoch 169/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0104 - val_loss: 0.0117\n",
            "Epoch 170/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0104 - val_loss: 0.0115\n",
            "Epoch 171/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0103 - val_loss: 0.0115\n",
            "Epoch 172/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0105 - val_loss: 0.0117\n",
            "Epoch 173/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0104 - val_loss: 0.0115\n",
            "Epoch 174/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0103 - val_loss: 0.0115\n",
            "Epoch 175/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0104 - val_loss: 0.0118\n",
            "Epoch 176/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0104 - val_loss: 0.0115\n",
            "Epoch 177/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0102 - val_loss: 0.0114\n",
            "Epoch 178/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0104 - val_loss: 0.0115\n",
            "Epoch 179/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0103 - val_loss: 0.0115\n",
            "Epoch 180/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0103 - val_loss: 0.0115\n",
            "Epoch 181/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0102 - val_loss: 0.0116\n",
            "Epoch 182/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0103 - val_loss: 0.0116\n",
            "Epoch 183/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0103 - val_loss: 0.0116\n",
            "Epoch 184/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0103 - val_loss: 0.0114\n",
            "Epoch 185/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0103 - val_loss: 0.0116\n",
            "Epoch 186/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0102 - val_loss: 0.0115\n",
            "Epoch 187/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0102 - val_loss: 0.0116\n",
            "Epoch 188/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0102 - val_loss: 0.0115\n",
            "Epoch 189/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0103 - val_loss: 0.0115\n",
            "Epoch 190/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0102 - val_loss: 0.0115\n",
            "Epoch 191/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0103 - val_loss: 0.0119\n",
            "Epoch 192/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0103 - val_loss: 0.0114\n",
            "Epoch 193/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0101 - val_loss: 0.0114\n",
            "Epoch 194/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0101 - val_loss: 0.0114\n",
            "Epoch 195/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0103 - val_loss: 0.0115\n",
            "Epoch 196/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0102 - val_loss: 0.0114\n",
            "Epoch 197/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0101 - val_loss: 0.0116\n",
            "Epoch 198/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0101 - val_loss: 0.0116\n",
            "Epoch 199/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0101 - val_loss: 0.0114\n",
            "Epoch 200/200\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0101 - val_loss: 0.0115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qRC1i7g-umg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = autoencoder.predict(val_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i25PcCi--322",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "57b9d626-f512-4839-c752-e216a3b78c1f"
      },
      "source": [
        "plt.imshow(pred[1].reshape(28, 28), cmap='gray')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fb1c0282ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bQjdhpJ-4Ul",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "f697f7f7-8d37-4f1c-c83f-73dfaec08a03"
      },
      "source": [
        "plt.imshow(val_x[1].reshape(28, 28), cmap='gray')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fb1c0330588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9N/vDiRZAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fg0yqMd2_h4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(train_x, train_y), (val_x, val_y) = fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiikhwg7AWcK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "6226d85b-9c8e-435f-c093-43be304ccc16"
      },
      "source": [
        "%pylab inline\n",
        "import os\n",
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras.backend as K\n",
        "\n",
        "from time import time\n",
        "from sklearn.cluster import KMeans\n",
        "from keras import callbacks\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import Dense, Input, Conv2D, MaxPool2D, UpSampling2D\n",
        "from keras.initializers import VarianceScaling\n",
        "from keras.engine.topology import Layer, InputSpec\n",
        "\n",
        "from scipy.misc import imread\n",
        "from sklearn.metrics import accuracy_score, normalized_mutual_info_score"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['imread', 'time']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adaLpX3hAHSU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from imgaug import augmenters as iaa\n",
        "seq = iaa.Sequential([iaa.SaltAndPepper(0.2)])\n",
        "\n",
        "train_x_aug = seq.augment_images(train_x)\n",
        "val_x_aug = seq.augment_images(val_x)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpV39PoLAI_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x = train_x/255.\n",
        "val_x = val_x/255.\n",
        "\n",
        "train_x = train_x.reshape(-1, 28, 28, 1)\n",
        "val_x = val_x.reshape(-1, 28, 28, 1)\n",
        "\n",
        "train_x_aug = train_x_aug/255.\n",
        "val_x_aug = val_x_aug/255.\n",
        "\n",
        "train_x_aug = train_x_aug.reshape(-1, 28, 28, 1)\n",
        "val_x_aug = val_x_aug.reshape(-1, 28, 28, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZpQ-dzQANRz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this is our input placeholder\n",
        "input_img = Input(shape=(28, 28, 1))\n",
        "\n",
        "# \"encoded\" is the encoded representation of the input\n",
        "encoded = Conv2D(64, (3, 3), activation='relu', padding='same')(input_img)\n",
        "encoded = MaxPool2D((2, 2), padding='same')(encoded)\n",
        "encoded = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
        "encoded = MaxPool2D((2, 2), padding='same')(encoded)\n",
        "encoded = Conv2D(16, (3, 3), activation='relu', padding='same')(encoded)\n",
        "encoded = MaxPool2D((2, 2), padding='same')(encoded)\n",
        "\n",
        "# \"decoded\" is the lossy reconstruction of the input\n",
        "decoded = Conv2D(16, (3, 3), activation='relu', padding='same')(encoded)\n",
        "decoded = UpSampling2D((2, 2))(decoded)\n",
        "decoded = Conv2D(32, (3, 3), activation='relu', padding='same')(decoded)\n",
        "decoded = UpSampling2D((2, 2))(decoded)\n",
        "decoded = Conv2D(64, (3, 3), activation='relu')(decoded)\n",
        "decoded = UpSampling2D((2, 2))(decoded)\n",
        "decoded = Conv2D(1, (3, 3), padding='same')(decoded)\n",
        "\n",
        "# this model maps an input to its reconstruction\n",
        "autoencoder = Model(input_img, decoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyPBLUxoARM8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "outputId": "8884b803-c00d-48e0-ef54-2b574a00c1d8"
      },
      "source": [
        "autoencoder.summary()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         (None, 28, 28, 1)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 28, 28, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 14, 14, 32)        18464     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 7, 7, 16)          4624      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 4, 4, 16)          2320      \n",
            "_________________________________________________________________\n",
            "up_sampling2d_4 (UpSampling2 (None, 8, 8, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 8, 8, 32)          4640      \n",
            "_________________________________________________________________\n",
            "up_sampling2d_5 (UpSampling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 14, 14, 64)        18496     \n",
            "_________________________________________________________________\n",
            "up_sampling2d_6 (UpSampling2 (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 28, 28, 1)         577       \n",
            "=================================================================\n",
            "Total params: 49,761\n",
            "Trainable params: 49,761\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Konb8Uq0ATLn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  this model maps an input to its encoded representation\n",
        "encoder = Model(input_img, encoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE0BUTLrAe72",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "autoencoder.compile(optimizer='adam', loss='mse')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljbq959NAg0Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukY5huEbAtYH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4776
        },
        "outputId": "718b7212-342c-4fb3-eab6-6492da929c66"
      },
      "source": [
        "train_history = autoencoder.fit(train_x_aug, train_x, epochs=500, batch_size=2048, validation_data=(val_x_aug, val_x), callbacks=[estop])"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/500\n",
            "60000/60000 [==============================] - 10s 165us/step - loss: 0.0835 - val_loss: 0.0491\n",
            "Epoch 2/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0402 - val_loss: 0.0339\n",
            "Epoch 3/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0308 - val_loss: 0.0280\n",
            "Epoch 4/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0265 - val_loss: 0.0250\n",
            "Epoch 5/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0242 - val_loss: 0.0233\n",
            "Epoch 6/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0228 - val_loss: 0.0223\n",
            "Epoch 7/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0218 - val_loss: 0.0225\n",
            "Epoch 8/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0212 - val_loss: 0.0209\n",
            "Epoch 9/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0205 - val_loss: 0.0204\n",
            "Epoch 10/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0201 - val_loss: 0.0197\n",
            "Epoch 11/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0196 - val_loss: 0.0194\n",
            "Epoch 12/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0192 - val_loss: 0.0191\n",
            "Epoch 13/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0188 - val_loss: 0.0193\n",
            "Epoch 14/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0185 - val_loss: 0.0184\n",
            "Epoch 15/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0183 - val_loss: 0.0180\n",
            "Epoch 16/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0179 - val_loss: 0.0179\n",
            "Epoch 17/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0177 - val_loss: 0.0175\n",
            "Epoch 18/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0175 - val_loss: 0.0175\n",
            "Epoch 19/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0172 - val_loss: 0.0172\n",
            "Epoch 20/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0171 - val_loss: 0.0169\n",
            "Epoch 21/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0169 - val_loss: 0.0168\n",
            "Epoch 22/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0166 - val_loss: 0.0171\n",
            "Epoch 23/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0165 - val_loss: 0.0163\n",
            "Epoch 24/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0164 - val_loss: 0.0162\n",
            "Epoch 25/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0162 - val_loss: 0.0160\n",
            "Epoch 26/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0160 - val_loss: 0.0160\n",
            "Epoch 27/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0158 - val_loss: 0.0160\n",
            "Epoch 28/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0158 - val_loss: 0.0157\n",
            "Epoch 29/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0155 - val_loss: 0.0157\n",
            "Epoch 30/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0157 - val_loss: 0.0154\n",
            "Epoch 31/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0152 - val_loss: 0.0152\n",
            "Epoch 32/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0151 - val_loss: 0.0153\n",
            "Epoch 33/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0153 - val_loss: 0.0150\n",
            "Epoch 34/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0149 - val_loss: 0.0149\n",
            "Epoch 35/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0149 - val_loss: 0.0158\n",
            "Epoch 36/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0149 - val_loss: 0.0147\n",
            "Epoch 37/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0146 - val_loss: 0.0146\n",
            "Epoch 38/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0145 - val_loss: 0.0157\n",
            "Epoch 39/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0149 - val_loss: 0.0146\n",
            "Epoch 40/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0143 - val_loss: 0.0144\n",
            "Epoch 41/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0143 - val_loss: 0.0143\n",
            "Epoch 42/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0142 - val_loss: 0.0144\n",
            "Epoch 43/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0142 - val_loss: 0.0142\n",
            "Epoch 44/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0141 - val_loss: 0.0140\n",
            "Epoch 45/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0140 - val_loss: 0.0144\n",
            "Epoch 46/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0140 - val_loss: 0.0141\n",
            "Epoch 47/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0139 - val_loss: 0.0141\n",
            "Epoch 48/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0139 - val_loss: 0.0141\n",
            "Epoch 49/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0139 - val_loss: 0.0138\n",
            "Epoch 50/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0137 - val_loss: 0.0138\n",
            "Epoch 51/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0137 - val_loss: 0.0140\n",
            "Epoch 52/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0137 - val_loss: 0.0136\n",
            "Epoch 53/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0136 - val_loss: 0.0136\n",
            "Epoch 54/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0135 - val_loss: 0.0135\n",
            "Epoch 55/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0134 - val_loss: 0.0134\n",
            "Epoch 56/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0135 - val_loss: 0.0135\n",
            "Epoch 57/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0134 - val_loss: 0.0133\n",
            "Epoch 58/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0133 - val_loss: 0.0133\n",
            "Epoch 59/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0134 - val_loss: 0.0133\n",
            "Epoch 60/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0132 - val_loss: 0.0132\n",
            "Epoch 61/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0132 - val_loss: 0.0132\n",
            "Epoch 62/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0133 - val_loss: 0.0134\n",
            "Epoch 63/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0130 - val_loss: 0.0130\n",
            "Epoch 64/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0130 - val_loss: 0.0130\n",
            "Epoch 65/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0131 - val_loss: 0.0130\n",
            "Epoch 66/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0130 - val_loss: 0.0129\n",
            "Epoch 67/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0129 - val_loss: 0.0133\n",
            "Epoch 68/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0129 - val_loss: 0.0128\n",
            "Epoch 69/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0127 - val_loss: 0.0128\n",
            "Epoch 70/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0130 - val_loss: 0.0128\n",
            "Epoch 71/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0127 - val_loss: 0.0128\n",
            "Epoch 72/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0127 - val_loss: 0.0127\n",
            "Epoch 73/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0127 - val_loss: 0.0126\n",
            "Epoch 74/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0127 - val_loss: 0.0126\n",
            "Epoch 75/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0126 - val_loss: 0.0126\n",
            "Epoch 76/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0126 - val_loss: 0.0126\n",
            "Epoch 77/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0125 - val_loss: 0.0126\n",
            "Epoch 78/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0126 - val_loss: 0.0125\n",
            "Epoch 79/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0124 - val_loss: 0.0126\n",
            "Epoch 80/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0126 - val_loss: 0.0124\n",
            "Epoch 81/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0123 - val_loss: 0.0124\n",
            "Epoch 82/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0124 - val_loss: 0.0124\n",
            "Epoch 83/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0124 - val_loss: 0.0125\n",
            "Epoch 84/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0123 - val_loss: 0.0123\n",
            "Epoch 85/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0123 - val_loss: 0.0129\n",
            "Epoch 86/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0124 - val_loss: 0.0122\n",
            "Epoch 87/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0122 - val_loss: 0.0124\n",
            "Epoch 88/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0123 - val_loss: 0.0122\n",
            "Epoch 89/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0122 - val_loss: 0.0122\n",
            "Epoch 90/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0121 - val_loss: 0.0123\n",
            "Epoch 91/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0121 - val_loss: 0.0121\n",
            "Epoch 92/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0121 - val_loss: 0.0125\n",
            "Epoch 93/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0121 - val_loss: 0.0121\n",
            "Epoch 94/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0120 - val_loss: 0.0122\n",
            "Epoch 95/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0122 - val_loss: 0.0120\n",
            "Epoch 96/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0119 - val_loss: 0.0121\n",
            "Epoch 97/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0119 - val_loss: 0.0121\n",
            "Epoch 98/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0120 - val_loss: 0.0123\n",
            "Epoch 99/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0120 - val_loss: 0.0120\n",
            "Epoch 100/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0119 - val_loss: 0.0119\n",
            "Epoch 101/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0118 - val_loss: 0.0119\n",
            "Epoch 102/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0120 - val_loss: 0.0119\n",
            "Epoch 103/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0118 - val_loss: 0.0119\n",
            "Epoch 104/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0118 - val_loss: 0.0129\n",
            "Epoch 105/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0119 - val_loss: 0.0118\n",
            "Epoch 106/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0117 - val_loss: 0.0119\n",
            "Epoch 107/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0120 - val_loss: 0.0119\n",
            "Epoch 108/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0117 - val_loss: 0.0118\n",
            "Epoch 109/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0117 - val_loss: 0.0118\n",
            "Epoch 110/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0117 - val_loss: 0.0117\n",
            "Epoch 111/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0116 - val_loss: 0.0117\n",
            "Epoch 112/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0117 - val_loss: 0.0117\n",
            "Epoch 113/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0117 - val_loss: 0.0118\n",
            "Epoch 114/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0116 - val_loss: 0.0116\n",
            "Epoch 115/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0117 - val_loss: 0.0120\n",
            "Epoch 116/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0115 - val_loss: 0.0116\n",
            "Epoch 117/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0116 - val_loss: 0.0117\n",
            "Epoch 118/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0115 - val_loss: 0.0115\n",
            "Epoch 119/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0115 - val_loss: 0.0117\n",
            "Epoch 120/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0115 - val_loss: 0.0116\n",
            "Epoch 121/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0115 - val_loss: 0.0116\n",
            "Epoch 122/500\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.0115 - val_loss: 0.0115\n",
            "Epoch 123/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0114 - val_loss: 0.0123\n",
            "Epoch 124/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0116 - val_loss: 0.0115\n",
            "Epoch 125/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0114 - val_loss: 0.0114\n",
            "Epoch 126/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0114 - val_loss: 0.0114\n",
            "Epoch 127/500\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.0114 - val_loss: 0.0114\n",
            "Epoch 128/500\n",
            "51200/60000 [========================>.....] - ETA: 0s - loss: 0.0113"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-bc87d6745e09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x_aug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_x_aug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y90pRBdfAva3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = autoencoder.predict(val_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-cD4RwrAxyG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "6eddce07-fa0d-4f09-85cc-ea53b0b8dfd6"
      },
      "source": [
        "plt.imshow(val_x[0].reshape(28, 28), cmap='gray')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fb17022ff60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au8JiORIA0Bc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "0ef46511-012d-4ba7-9e72-091803c728c5"
      },
      "source": [
        "plt.imshow(val_x_aug[0].reshape(28, 28), cmap='gray')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fb1702085c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aqs0tv0IA57T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "543aa97a-695b-4d68-de17-6594aa40b0ab"
      },
      "source": [
        "plt.imshow(pred[0].reshape(28, 28), cmap='gray')\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fb1701dcbe0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtQpwd-nA8D6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import fashion_mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TekUNW8Bkts",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "1ff7c93c-a48e-46c1-c3ba-5b502ff02365"
      },
      "source": [
        "%pylab inline\n",
        "import os\n",
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras.backend as K\n",
        "\n",
        "from time import time\n",
        "from sklearn.cluster import KMeans\n",
        "from keras import callbacks\n",
        "from keras.models import Model, Sequential\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import Dense, Input, Lambda, Layer, Add, Multiply\n",
        "from keras.initializers import VarianceScaling\n",
        "from keras.engine.topology import Layer, InputSpec\n",
        "\n",
        "from scipy.misc import imread\n",
        "from sklearn.metrics import accuracy_score, normalized_mutual_info_score"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['imread', 'time']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WejTydlSBmTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(train_x, train_y), (val_x, val_y) = fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuxHYVbLBoXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x = train_x/255.\n",
        "val_x = val_x/255.\n",
        "\n",
        "train_x = train_x.reshape(-1, 784)\n",
        "val_x = val_x.reshape(-1, 784)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjjK6VPSBs-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this is our input placeholder\n",
        "input_img = Input(shape=(784,))\n",
        "\n",
        "# \"encoded\" is the encoded representation of the input\n",
        "encoded = Dense(500, activation='relu')(input_img)\n",
        "\n",
        "z_mu = Dense(10)(encoded)\n",
        "z_log_sigma = Dense(10)(encoded)\n",
        "\n",
        "class KLDivergenceLayer(Layer):\n",
        "\n",
        "    \"\"\" Identity transform layer that adds KL divergence\n",
        "    to the final model loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.is_placeholder = True\n",
        "        super(KLDivergenceLayer, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "        mu, log_sigma = inputs\n",
        "\n",
        "        kl_batch = - .5 * K.sum(1 + log_sigma -\n",
        "                                K.square(mu) -\n",
        "                                K.exp(log_sigma), axis=-1)\n",
        "\n",
        "        self.add_loss(K.mean(kl_batch), inputs=inputs)\n",
        "\n",
        "        return inputs\n",
        "\n",
        "z_mu, z_log_sigma = KLDivergenceLayer()([z_mu, z_log_sigma])\n",
        "z_sigma = Lambda(lambda t: K.exp(.5*t))(z_log_sigma)\n",
        "\n",
        "eps = Input(tensor=K.random_normal(shape=(K.shape(input_img)[0],10)))\n",
        "z_eps = Multiply()([z_sigma, eps])\n",
        "z = Add()([z_mu, z_eps])\n",
        "\n",
        "decoder = Sequential([\n",
        "    Dense(500, input_dim=10, activation='relu'),\n",
        "    Dense(784, activation='sigmoid')\n",
        "])\n",
        "\n",
        "decoded = decoder(z)\n",
        "\n",
        "# this model maps an input to its reconstruction\n",
        "autoencoder = Model([input_img, eps], decoded)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01zRKyZzBthw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "9516baa3-53e0-4251-be8a-912cb006a625"
      },
      "source": [
        "autoencoder.summary()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            (None, 784)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_17 (Dense)                (None, 500)          392500      input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_18 (Dense)                (None, 10)           5010        dense_17[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_19 (Dense)                (None, 10)           5010        dense_17[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "kl_divergence_layer_1 (KLDiverg [(None, 10), (None,  0           dense_18[0][0]                   \n",
            "                                                                 dense_19[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 10)           0           kl_divergence_layer_1[0][1]      \n",
            "__________________________________________________________________________________________________\n",
            "input_7 (InputLayer)            (None, 10)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "multiply_1 (Multiply)           (None, 10)           0           lambda_1[0][0]                   \n",
            "                                                                 input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 10)           0           kl_divergence_layer_1[0][0]      \n",
            "                                                                 multiply_1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "sequential_1 (Sequential)       (None, 784)          398284      add_1[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 800,804\n",
            "Trainable params: 800,804\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLE60UE6B6Yb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nll(y_true, y_pred):\n",
        "    \"\"\" Negative log likelihood (Bernoulli). \"\"\"\n",
        "\n",
        "    # keras.losses.binary_crossentropy gives the mean\n",
        "    # over the last axis. we require the sum\n",
        "    return K.sum(K.binary_crossentropy(y_true, y_pred), axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Urh3EirB-Vd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "autoencoder.compile(optimizer='adam', loss=nll)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lelBQuT5CAQV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2WVc5QfCB2O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10506
        },
        "outputId": "3a5f193a-21f1-4c49-bce7-f1832a931882"
      },
      "source": [
        "train_history = autoencoder.fit(train_x, train_x, epochs=500, batch_size=2048, validation_data=(val_x, val_x), callbacks=[estop])"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/500\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 421.2372 - val_loss: 337.7020\n",
            "Epoch 2/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 315.6732 - val_loss: 300.6069\n",
            "Epoch 3/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 290.2850 - val_loss: 286.3036\n",
            "Epoch 4/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 277.9142 - val_loss: 274.5575\n",
            "Epoch 5/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 270.4135 - val_loss: 269.5258\n",
            "Epoch 6/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 266.5358 - val_loss: 267.4094\n",
            "Epoch 7/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 263.6435 - val_loss: 263.8795\n",
            "Epoch 8/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 261.2386 - val_loss: 263.1069\n",
            "Epoch 9/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 259.4858 - val_loss: 259.8763\n",
            "Epoch 10/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 257.5148 - val_loss: 258.4774\n",
            "Epoch 11/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 256.3664 - val_loss: 257.1812\n",
            "Epoch 12/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 255.3493 - val_loss: 257.0117\n",
            "Epoch 13/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 254.6405 - val_loss: 256.5556\n",
            "Epoch 14/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 253.5532 - val_loss: 254.6773\n",
            "Epoch 15/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 252.6829 - val_loss: 254.1772\n",
            "Epoch 16/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 252.0632 - val_loss: 253.6385\n",
            "Epoch 17/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 251.4672 - val_loss: 252.8971\n",
            "Epoch 18/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 250.9522 - val_loss: 252.5341\n",
            "Epoch 19/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 250.5425 - val_loss: 252.0763\n",
            "Epoch 20/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 250.1087 - val_loss: 252.5021\n",
            "Epoch 21/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 249.8113 - val_loss: 251.1805\n",
            "Epoch 22/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 249.2892 - val_loss: 250.7667\n",
            "Epoch 23/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 249.0290 - val_loss: 250.5633\n",
            "Epoch 24/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 248.7059 - val_loss: 250.3530\n",
            "Epoch 25/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 248.3701 - val_loss: 250.0580\n",
            "Epoch 26/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 248.0694 - val_loss: 250.0262\n",
            "Epoch 27/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 247.9764 - val_loss: 249.5081\n",
            "Epoch 28/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 247.6184 - val_loss: 249.3096\n",
            "Epoch 29/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 247.3432 - val_loss: 248.8646\n",
            "Epoch 30/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 246.9997 - val_loss: 248.6744\n",
            "Epoch 31/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 247.0011 - val_loss: 248.5267\n",
            "Epoch 32/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 246.6828 - val_loss: 248.4646\n",
            "Epoch 33/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 246.5073 - val_loss: 248.0787\n",
            "Epoch 34/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 246.2687 - val_loss: 248.1217\n",
            "Epoch 35/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 246.4959 - val_loss: 247.7406\n",
            "Epoch 36/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 245.9101 - val_loss: 247.7740\n",
            "Epoch 37/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 245.9005 - val_loss: 247.5758\n",
            "Epoch 38/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 245.6523 - val_loss: 247.6769\n",
            "Epoch 39/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 245.6144 - val_loss: 248.0013\n",
            "Epoch 40/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 245.4853 - val_loss: 247.2558\n",
            "Epoch 41/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 245.3378 - val_loss: 247.5770\n",
            "Epoch 42/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 245.2028 - val_loss: 247.3237\n",
            "Epoch 43/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 244.9688 - val_loss: 246.7652\n",
            "Epoch 44/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 244.9585 - val_loss: 246.5510\n",
            "Epoch 45/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 244.8497 - val_loss: 247.4856\n",
            "Epoch 46/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 244.9611 - val_loss: 246.8101\n",
            "Epoch 47/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 244.6108 - val_loss: 246.2119\n",
            "Epoch 48/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 244.4818 - val_loss: 246.2780\n",
            "Epoch 49/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 244.4760 - val_loss: 246.0992\n",
            "Epoch 50/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 244.2778 - val_loss: 246.0313\n",
            "Epoch 51/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 244.1887 - val_loss: 246.2065\n",
            "Epoch 52/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 243.9989 - val_loss: 245.8006\n",
            "Epoch 53/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 243.9686 - val_loss: 245.9062\n",
            "Epoch 54/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 243.9700 - val_loss: 245.7401\n",
            "Epoch 55/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 243.7623 - val_loss: 245.9042\n",
            "Epoch 56/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 243.8505 - val_loss: 245.5534\n",
            "Epoch 57/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 243.5902 - val_loss: 245.3795\n",
            "Epoch 58/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 243.5508 - val_loss: 245.6130\n",
            "Epoch 59/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 243.5722 - val_loss: 245.3923\n",
            "Epoch 60/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 243.4481 - val_loss: 245.4502\n",
            "Epoch 61/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 243.3757 - val_loss: 245.2044\n",
            "Epoch 62/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 243.2943 - val_loss: 245.0139\n",
            "Epoch 63/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 243.2941 - val_loss: 245.1214\n",
            "Epoch 64/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 243.0380 - val_loss: 245.0001\n",
            "Epoch 65/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 243.0917 - val_loss: 244.9475\n",
            "Epoch 66/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 242.9591 - val_loss: 244.9229\n",
            "Epoch 67/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 243.1318 - val_loss: 245.1667\n",
            "Epoch 68/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 242.9600 - val_loss: 244.9436\n",
            "Epoch 69/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 242.8056 - val_loss: 244.8509\n",
            "Epoch 70/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 242.7236 - val_loss: 244.6734\n",
            "Epoch 71/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 242.6545 - val_loss: 244.7161\n",
            "Epoch 72/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 242.6492 - val_loss: 244.4236\n",
            "Epoch 73/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 242.6102 - val_loss: 244.4671\n",
            "Epoch 74/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 242.6785 - val_loss: 244.3576\n",
            "Epoch 75/500\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 242.3976 - val_loss: 244.2617\n",
            "Epoch 76/500\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 242.3708 - val_loss: 244.4613\n",
            "Epoch 77/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 242.4574 - val_loss: 244.2041\n",
            "Epoch 78/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 242.2223 - val_loss: 244.2779\n",
            "Epoch 79/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 242.3079 - val_loss: 244.1202\n",
            "Epoch 80/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 242.4418 - val_loss: 244.1188\n",
            "Epoch 81/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 242.0935 - val_loss: 243.9570\n",
            "Epoch 82/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 242.0287 - val_loss: 244.1228\n",
            "Epoch 83/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 242.0178 - val_loss: 243.9686\n",
            "Epoch 84/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 242.1136 - val_loss: 244.2045\n",
            "Epoch 85/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 242.0254 - val_loss: 244.0861\n",
            "Epoch 86/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 241.9499 - val_loss: 243.8387\n",
            "Epoch 87/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 241.8442 - val_loss: 243.8426\n",
            "Epoch 88/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 241.8771 - val_loss: 244.0703\n",
            "Epoch 89/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 241.7850 - val_loss: 243.7338\n",
            "Epoch 90/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 241.8276 - val_loss: 243.6645\n",
            "Epoch 91/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 241.6204 - val_loss: 243.6586\n",
            "Epoch 92/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 241.6061 - val_loss: 243.5588\n",
            "Epoch 93/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 241.6134 - val_loss: 243.6075\n",
            "Epoch 94/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 241.5558 - val_loss: 243.4826\n",
            "Epoch 95/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 241.5183 - val_loss: 243.7509\n",
            "Epoch 96/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 241.4384 - val_loss: 243.3291\n",
            "Epoch 97/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 241.3663 - val_loss: 243.3868\n",
            "Epoch 98/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 241.3167 - val_loss: 243.3301\n",
            "Epoch 99/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 241.3128 - val_loss: 243.3451\n",
            "Epoch 100/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 241.2346 - val_loss: 243.2314\n",
            "Epoch 101/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 241.3890 - val_loss: 243.6351\n",
            "Epoch 102/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 241.1826 - val_loss: 243.1565\n",
            "Epoch 103/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 241.1804 - val_loss: 243.2354\n",
            "Epoch 104/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 241.2022 - val_loss: 243.1738\n",
            "Epoch 105/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 241.1131 - val_loss: 243.1010\n",
            "Epoch 106/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 241.1083 - val_loss: 243.1931\n",
            "Epoch 107/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 241.0036 - val_loss: 243.0532\n",
            "Epoch 108/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 240.9751 - val_loss: 243.0176\n",
            "Epoch 109/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 240.9271 - val_loss: 242.9689\n",
            "Epoch 110/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 240.8701 - val_loss: 243.6054\n",
            "Epoch 111/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 241.1980 - val_loss: 242.9768\n",
            "Epoch 112/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 240.7831 - val_loss: 242.7397\n",
            "Epoch 113/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 240.9223 - val_loss: 242.8290\n",
            "Epoch 114/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 240.7422 - val_loss: 242.9722\n",
            "Epoch 115/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 240.6917 - val_loss: 242.6681\n",
            "Epoch 116/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 240.6950 - val_loss: 242.6842\n",
            "Epoch 117/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 240.7516 - val_loss: 242.9658\n",
            "Epoch 118/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 240.6439 - val_loss: 242.5854\n",
            "Epoch 119/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 240.5304 - val_loss: 242.5476\n",
            "Epoch 120/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 240.5222 - val_loss: 242.6711\n",
            "Epoch 121/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 240.7126 - val_loss: 242.5152\n",
            "Epoch 122/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 240.5041 - val_loss: 242.5909\n",
            "Epoch 123/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 240.4173 - val_loss: 242.5385\n",
            "Epoch 124/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 240.4629 - val_loss: 242.5155\n",
            "Epoch 125/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 240.4491 - val_loss: 242.4005\n",
            "Epoch 126/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 240.4122 - val_loss: 242.6291\n",
            "Epoch 127/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 240.4409 - val_loss: 242.3893\n",
            "Epoch 128/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 240.4198 - val_loss: 242.5319\n",
            "Epoch 129/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 240.2864 - val_loss: 242.3383\n",
            "Epoch 130/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 240.2476 - val_loss: 242.4243\n",
            "Epoch 131/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 240.2939 - val_loss: 242.2146\n",
            "Epoch 132/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 240.2897 - val_loss: 242.2155\n",
            "Epoch 133/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 240.2126 - val_loss: 242.2723\n",
            "Epoch 134/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 240.1287 - val_loss: 242.2302\n",
            "Epoch 135/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 240.1603 - val_loss: 242.2156\n",
            "Epoch 136/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 240.1141 - val_loss: 242.1819\n",
            "Epoch 137/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 240.1263 - val_loss: 242.3871\n",
            "Epoch 138/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 240.1203 - val_loss: 242.1203\n",
            "Epoch 139/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.9999 - val_loss: 242.0154\n",
            "Epoch 140/500\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 240.0093 - val_loss: 242.1405\n",
            "Epoch 141/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 240.0265 - val_loss: 243.0806\n",
            "Epoch 142/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 240.0998 - val_loss: 242.0231\n",
            "Epoch 143/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.9054 - val_loss: 241.9961\n",
            "Epoch 144/500\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 239.9499 - val_loss: 242.0015\n",
            "Epoch 145/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.9187 - val_loss: 242.0921\n",
            "Epoch 146/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.8573 - val_loss: 241.9253\n",
            "Epoch 147/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.7759 - val_loss: 241.8148\n",
            "Epoch 148/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.7806 - val_loss: 242.4705\n",
            "Epoch 149/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.9317 - val_loss: 241.9162\n",
            "Epoch 150/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.7583 - val_loss: 241.8712\n",
            "Epoch 151/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.8143 - val_loss: 241.8516\n",
            "Epoch 152/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.7651 - val_loss: 242.0187\n",
            "Epoch 153/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.6879 - val_loss: 241.7988\n",
            "Epoch 154/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.6746 - val_loss: 242.0038\n",
            "Epoch 155/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.7589 - val_loss: 241.8342\n",
            "Epoch 156/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.6104 - val_loss: 241.8605\n",
            "Epoch 157/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.5745 - val_loss: 241.6861\n",
            "Epoch 158/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.6384 - val_loss: 241.6774\n",
            "Epoch 159/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.5480 - val_loss: 241.8603\n",
            "Epoch 160/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.5372 - val_loss: 241.5535\n",
            "Epoch 161/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.5065 - val_loss: 243.1293\n",
            "Epoch 162/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.9311 - val_loss: 241.7065\n",
            "Epoch 163/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.5515 - val_loss: 241.7390\n",
            "Epoch 164/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.4346 - val_loss: 241.5670\n",
            "Epoch 165/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.4362 - val_loss: 241.6001\n",
            "Epoch 166/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.3901 - val_loss: 241.8373\n",
            "Epoch 167/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.3984 - val_loss: 241.5082\n",
            "Epoch 168/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.3616 - val_loss: 241.5396\n",
            "Epoch 169/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.6314 - val_loss: 241.9598\n",
            "Epoch 170/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.4707 - val_loss: 241.5136\n",
            "Epoch 171/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.3127 - val_loss: 241.5223\n",
            "Epoch 172/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.2787 - val_loss: 241.4696\n",
            "Epoch 173/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.2690 - val_loss: 241.5771\n",
            "Epoch 174/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.2489 - val_loss: 241.4369\n",
            "Epoch 175/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.3275 - val_loss: 241.4462\n",
            "Epoch 176/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.2428 - val_loss: 241.4853\n",
            "Epoch 177/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.2034 - val_loss: 241.5414\n",
            "Epoch 178/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.2731 - val_loss: 241.3513\n",
            "Epoch 179/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.2290 - val_loss: 241.4163\n",
            "Epoch 180/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.1767 - val_loss: 241.2679\n",
            "Epoch 181/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.1131 - val_loss: 241.5472\n",
            "Epoch 182/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.2067 - val_loss: 241.4009\n",
            "Epoch 183/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.1471 - val_loss: 241.5091\n",
            "Epoch 184/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.1964 - val_loss: 241.2745\n",
            "Epoch 185/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.0112 - val_loss: 241.2907\n",
            "Epoch 186/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.0303 - val_loss: 241.3806\n",
            "Epoch 187/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.1573 - val_loss: 241.2389\n",
            "Epoch 188/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.9967 - val_loss: 241.1922\n",
            "Epoch 189/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.0357 - val_loss: 241.2354\n",
            "Epoch 190/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.0272 - val_loss: 241.2978\n",
            "Epoch 191/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.0212 - val_loss: 241.2503\n",
            "Epoch 192/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 239.0459 - val_loss: 241.2102\n",
            "Epoch 193/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.9260 - val_loss: 241.2436\n",
            "Epoch 194/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.9440 - val_loss: 241.2610\n",
            "Epoch 195/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.9541 - val_loss: 241.1391\n",
            "Epoch 196/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.8783 - val_loss: 241.1590\n",
            "Epoch 197/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.9108 - val_loss: 241.0563\n",
            "Epoch 198/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.8709 - val_loss: 241.1677\n",
            "Epoch 199/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.8838 - val_loss: 241.1398\n",
            "Epoch 200/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.8526 - val_loss: 241.1280\n",
            "Epoch 201/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.9053 - val_loss: 241.2297\n",
            "Epoch 202/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.8594 - val_loss: 241.2353\n",
            "Epoch 203/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.8629 - val_loss: 241.0113\n",
            "Epoch 204/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.8197 - val_loss: 241.0371\n",
            "Epoch 205/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.7373 - val_loss: 241.2559\n",
            "Epoch 206/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.7883 - val_loss: 241.0767\n",
            "Epoch 207/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.7233 - val_loss: 241.0957\n",
            "Epoch 208/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.8289 - val_loss: 241.0781\n",
            "Epoch 209/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.7200 - val_loss: 241.2341\n",
            "Epoch 210/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.6893 - val_loss: 240.9608\n",
            "Epoch 211/500\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 238.6836 - val_loss: 240.8678\n",
            "Epoch 212/500\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 238.6673 - val_loss: 241.1259\n",
            "Epoch 213/500\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 238.6946 - val_loss: 241.0624\n",
            "Epoch 214/500\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 238.8333 - val_loss: 240.9774\n",
            "Epoch 215/500\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 238.7097 - val_loss: 240.9437\n",
            "Epoch 216/500\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 238.5665 - val_loss: 240.8975\n",
            "Epoch 217/500\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 238.7721 - val_loss: 241.0072\n",
            "Epoch 218/500\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 238.5427 - val_loss: 240.8786\n",
            "Epoch 219/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.6405 - val_loss: 240.8599\n",
            "Epoch 220/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.5215 - val_loss: 240.8128\n",
            "Epoch 221/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.5390 - val_loss: 240.8832\n",
            "Epoch 222/500\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 238.6777 - val_loss: 240.7761\n",
            "Epoch 223/500\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 238.5412 - val_loss: 240.8641\n",
            "Epoch 224/500\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 238.5353 - val_loss: 240.8665\n",
            "Epoch 225/500\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 238.5371 - val_loss: 240.9882\n",
            "Epoch 226/500\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 238.5905 - val_loss: 240.9247\n",
            "Epoch 227/500\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 238.5365 - val_loss: 240.7104\n",
            "Epoch 228/500\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 238.5105 - val_loss: 240.7102\n",
            "Epoch 229/500\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 238.4327 - val_loss: 240.8283\n",
            "Epoch 230/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.5513 - val_loss: 240.9811\n",
            "Epoch 231/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.5955 - val_loss: 240.6637\n",
            "Epoch 232/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.4011 - val_loss: 240.7953\n",
            "Epoch 233/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.4988 - val_loss: 240.8174\n",
            "Epoch 234/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.4309 - val_loss: 240.7688\n",
            "Epoch 235/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.3977 - val_loss: 240.8712\n",
            "Epoch 236/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.4066 - val_loss: 240.7082\n",
            "Epoch 237/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.3262 - val_loss: 240.6433\n",
            "Epoch 238/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.3834 - val_loss: 240.7379\n",
            "Epoch 239/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.3813 - val_loss: 240.6848\n",
            "Epoch 240/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.3322 - val_loss: 240.6725\n",
            "Epoch 241/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.3034 - val_loss: 240.9577\n",
            "Epoch 242/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.3453 - val_loss: 240.6969\n",
            "Epoch 243/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.3227 - val_loss: 240.6213\n",
            "Epoch 244/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.2726 - val_loss: 240.6418\n",
            "Epoch 245/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.2746 - val_loss: 240.5576\n",
            "Epoch 246/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.2917 - val_loss: 241.0701\n",
            "Epoch 247/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.4275 - val_loss: 240.7425\n",
            "Epoch 248/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.2448 - val_loss: 240.5865\n",
            "Epoch 249/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.2635 - val_loss: 240.6894\n",
            "Epoch 250/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.2677 - val_loss: 240.6424\n",
            "Epoch 251/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.2812 - val_loss: 240.6265\n",
            "Epoch 252/500\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 238.1906 - val_loss: 240.5894\n",
            "Epoch 253/500\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 238.2041 - val_loss: 240.5378\n",
            "Epoch 254/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.2642 - val_loss: 240.7724\n",
            "Epoch 255/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.2213 - val_loss: 240.4973\n",
            "Epoch 256/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.2100 - val_loss: 240.5589\n",
            "Epoch 257/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.1198 - val_loss: 240.8183\n",
            "Epoch 258/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.1544 - val_loss: 240.5108\n",
            "Epoch 259/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.2131 - val_loss: 240.5794\n",
            "Epoch 260/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.1097 - val_loss: 240.5224\n",
            "Epoch 261/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.1606 - val_loss: 240.5745\n",
            "Epoch 262/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.2292 - val_loss: 240.5892\n",
            "Epoch 263/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.1234 - val_loss: 240.5162\n",
            "Epoch 264/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.1033 - val_loss: 240.5377\n",
            "Epoch 265/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.1620 - val_loss: 240.4345\n",
            "Epoch 266/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.0788 - val_loss: 240.7163\n",
            "Epoch 267/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.1272 - val_loss: 240.3987\n",
            "Epoch 268/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.1368 - val_loss: 240.5468\n",
            "Epoch 269/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.0491 - val_loss: 240.8041\n",
            "Epoch 270/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.0952 - val_loss: 240.5857\n",
            "Epoch 271/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.0196 - val_loss: 240.4668\n",
            "Epoch 272/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.0533 - val_loss: 240.4700\n",
            "Epoch 273/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.0504 - val_loss: 240.4226\n",
            "Epoch 274/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 237.9751 - val_loss: 240.3594\n",
            "Epoch 275/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 237.9386 - val_loss: 240.3772\n",
            "Epoch 276/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 237.9678 - val_loss: 240.4017\n",
            "Epoch 277/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 237.9981 - val_loss: 240.4542\n",
            "Epoch 278/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 238.0260 - val_loss: 240.3651\n",
            "Epoch 279/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 237.9629 - val_loss: 240.5648\n",
            "Epoch 280/500\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 238.0379 - val_loss: 240.5926\n",
            "Epoch 281/500\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 238.0039 - val_loss: 240.4087\n",
            "Epoch 282/500\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 238.0130 - val_loss: 240.4805\n",
            "Epoch 283/500\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 237.9820 - val_loss: 240.3232\n",
            "Epoch 284/500\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 237.9374 - val_loss: 240.5372\n",
            "Epoch 285/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 237.9690 - val_loss: 240.3948\n",
            "Epoch 286/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 237.8814 - val_loss: 240.2111\n",
            "Epoch 287/500\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 237.9718 - val_loss: 240.4336\n",
            "Epoch 288/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 237.8812 - val_loss: 240.2431\n",
            "Epoch 289/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 237.8366 - val_loss: 240.3546\n",
            "Epoch 290/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 237.9157 - val_loss: 240.2457\n",
            "Epoch 291/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 237.8925 - val_loss: 240.1695\n",
            "Epoch 292/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 237.8421 - val_loss: 240.3227\n",
            "Epoch 293/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 237.9450 - val_loss: 240.2773\n",
            "Epoch 294/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 237.8422 - val_loss: 240.4201\n",
            "Epoch 295/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 237.8907 - val_loss: 240.3907\n",
            "Epoch 296/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 237.8017 - val_loss: 240.3934\n",
            "Epoch 297/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 237.8625 - val_loss: 240.2079\n",
            "Epoch 298/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 237.7831 - val_loss: 240.3451\n",
            "Epoch 299/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 237.8130 - val_loss: 240.4108\n",
            "Epoch 300/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 237.8577 - val_loss: 240.2135\n",
            "Epoch 301/500\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 237.7931 - val_loss: 240.2302\n",
            "Epoch 00301: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srNtGBs6CEas",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = autoencoder.predict(val_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "By0yeIqUCGH_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "c71352d5-a83a-40b4-8cc3-e181a9cf8976"
      },
      "source": [
        "plt.imshow(pred[0].reshape(28, 28), cmap='gray')"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fb16f9d50f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4XZVPMQCHx1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "922544c0-4a0a-4ae2-cec9-13abd7c480d5"
      },
      "source": [
        "plt.imshow(val_x[0].reshape(28, 28), cmap='gray')"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fb16f926d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMtjgqPjCJcw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "076f6d11-5274-475a-e912-01d9a8cf76e2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "print(train_history.history.keys())\n",
        "# summarize history for accuracy\n",
        "#plt.plot(train_history.history['acc'])\n",
        "#plt.plot(train_history.history['val_acc'])\n",
        "#plt.title('model accuracy')\n",
        "#plt.ylabel('accuracy')\n",
        "#plt.xlabel('epoch')\n",
        "#plt.legend(['train', 'test'], loc='upper left')\n",
        "#plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(train_history.history['loss'])\n",
        "plt.plot(train_history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "#resnet , vgg-16"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['val_loss', 'loss'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRMpSQIiISFkKCBERKev/A0aulsUSrJX/AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M9sSCffCVV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}